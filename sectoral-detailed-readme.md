# Documentation Technique DÃ©taillÃ©e - Sectoral

> **Guide complet de setup, architecture et troubleshooting**

## ðŸ“‹ Table des MatiÃ¨res

1. [Architecture DÃ©taillÃ©e](#architecture-dÃ©taillÃ©e)
2. [Setup Environnement](#setup-environnement)
3. [Configuration AWS](#configuration-aws)
4. [DÃ©ploiement Terraform](#dÃ©ploiement-terraform)
5. [Configuration Airflow](#configuration-airflow)
6. [ModÃ¨les dbt](#modÃ¨les-dbt)
7. [Scripts Python](#scripts-python)
8. [Monitoring & Alertes](#monitoring--alertes)
9. [Troubleshooting](#troubleshooting)
10. [Bonnes Pratiques](#bonnes-pratiques)

---

## ðŸ—ï¸ Architecture DÃ©taillÃ©e

### Flux de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   APIs Externes â”‚    â”‚   Data Pipeline  â”‚    â”‚   Data Storage  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Yahoo Finance â”‚â”€â”€â”€â–¶â”‚ Python Ingestion â”‚â”€â”€â”€â–¶â”‚ S3 Raw Zone     â”‚
â”‚ â€¢ Alpha Vantage â”‚    â”‚ â€¢ Validation     â”‚    â”‚ â€¢ JSON/Parquet  â”‚
â”‚ â€¢ FRED API      â”‚    â”‚ â€¢ Enrichment     â”‚    â”‚ â€¢ Partitioning  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Orchestration â”‚    â”‚  Transformation  â”‚    â”‚  Data Warehouse â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Apache Airflow  â”‚â”€â”€â”€â–¶â”‚ dbt Core         â”‚â”€â”€â”€â–¶â”‚ Amazon Redshift â”‚
â”‚ â€¢ DAGs          â”‚    â”‚ â€¢ Staging        â”‚    â”‚ â€¢ Dimensional   â”‚
â”‚ â€¢ Scheduling    â”‚    â”‚ â€¢ Intermediate   â”‚    â”‚ â€¢ Star Schema   â”‚
â”‚ â€¢ Monitoring    â”‚    â”‚ â€¢ Marts          â”‚    â”‚ â€¢ Aggregations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants AWS

| Service | Usage | Configuration |
|---------|-------|---------------|
| **S3** | Data Lake | Buckets : raw/, processed/, logs/ |
| **Redshift** | Data Warehouse | dc2.large cluster (dev), ra3.xlplus (prod) |
| **MWAA** | Airflow Managed | Environment v2.5.1, Small instance |
| **IAM** | SÃ©curitÃ© | Roles : AirflowExecutionRole, dbtRole |
| **VPC** | RÃ©seau | Private subnets pour Redshift |
| **CloudWatch** | Monitoring | Logs + mÃ©triques custom |

---

## ðŸ› ï¸ Setup Environnement

### PrÃ©requis Locaux

```bash
# Python 3.8+
python --version

# AWS CLI v2
aws --version
aws configure

# Terraform
terraform --version

# Git
git --version
```

### Variables d'Environnement

```bash
# .env
export AWS_REGION=eu-west-1
export AWS_PROFILE=sectoral
export PROJECT_NAME=sectoral
export ENVIRONMENT=dev

# API Keys (optionnel pour APIs gratuites)
export ALPHA_VANTAGE_API_KEY=your_key_here
export FRED_API_KEY=your_key_here
```

### Installation Python

```bash
# Virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Requirements
pip install -r requirements.txt
```

**requirements.txt**
```
# Core
pandas>=1.5.0
numpy>=1.21.0
yfinance>=0.2.0
requests>=2.28.0

# AWS
boto3>=1.26.0
awswrangler>=3.0.0

# dbt
dbt-core>=1.4.0
dbt-redshift>=1.4.0

# Airflow (pour dÃ©veloppement local)
apache-airflow>=2.5.0
apache-airflow-providers-amazon>=7.0.0

# Utils
python-dotenv>=0.19.0
pydantic>=1.10.0
great-expectations>=0.15.0  # Data quality
```

---

## â˜ï¸ Configuration AWS

### 1. CrÃ©ation du Profil AWS

```bash
aws configure --profile sectoral
# AWS Access Key ID: AKIA...
# AWS Secret Access Key: ...
# Default region: eu-west-1
# Default output format: json
```

### 2. Permissions IAM Requises

**Policy : SectoralDataEngineer**
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:*",
                "redshift:*",
                "mwaa:*",
                "iam:PassRole",
                "logs:*",
                "cloudwatch:*"
            ],
            "Resource": "*"
        }
    ]
}
```

### 3. Validation Setup

```bash
# Test connexion
aws sts get-caller-identity --profile sectoral

# VÃ©rification rÃ©gion
aws configure get region --profile sectoral
```

---

## ðŸš€ DÃ©ploiement Terraform

### Structure Terraform

```
terraform/
â”œâ”€â”€ main.tf              # Configuration principale
â”œâ”€â”€ variables.tf         # Variables d'entrÃ©e
â”œâ”€â”€ outputs.tf          # Sorties (endpoints, ARNs)
â”œâ”€â”€ providers.tf        # Providers AWS
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ s3/             # Module S3 buckets
â”‚   â”œâ”€â”€ redshift/       # Module Redshift cluster
â”‚   â”œâ”€â”€ mwaa/           # Module Airflow
â”‚   â””â”€â”€ iam/            # Module IAM roles
â””â”€â”€ environments/
    â”œâ”€â”€ dev.tfvars      # Variables dev
    â””â”€â”€ prod.tfvars     # Variables prod
```

### Configuration Principale

**main.tf**
```hcl
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region  = var.aws_region
  profile = var.aws_profile
}

module "s3" {
  source = "./modules/s3"
  
  project_name = var.project_name
  environment  = var.environment
}

module "redshift" {
  source = "./modules/redshift"
  
  project_name    = var.project_name
  environment     = var.environment
  master_password = var.redshift_password
}

module "mwaa" {
  source = "./modules/mwaa"
  
  project_name = var.project_name
  environment  = var.environment
  s3_bucket    = module.s3.airflow_bucket_name
}
```

### DÃ©ploiement

```bash
cd terraform/

# Initialisation
terraform init

# Plan (dry-run)
terraform plan -var-file="environments/dev.tfvars"

# Application
terraform apply -var-file="environments/dev.tfvars"

# VÃ©rification
terraform output
```

### Variables Dev

**environments/dev.tfvars**
```hcl
aws_region = "eu-west-1"
aws_profile = "sectoral"
project_name = "sectoral"
environment = "dev"

# Redshift
redshift_node_type = "dc2.large"
redshift_cluster_type = "single-node"
redshift_password = "ChangeMe123!"

# MWAA
mwaa_environment_class = "mw1.small"
mwaa_max_workers = 2
```

---

## ðŸ”„ Configuration Airflow

### Structure DAGs

```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ daily_market_ingestion.py      # Pipeline quotidien
â”‚   â”œâ”€â”€ weekly_sector_analysis.py      # Analyse hebdomadaire
â”‚   â””â”€â”€ data_quality_checks.py         # Tests qualitÃ©
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ sectoral_operators.py          # Custom operators
â”‚   â””â”€â”€ sectoral_hooks.py               # Custom hooks
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ connections.json               # Connexions Airflow
â”‚   â””â”€â”€ variables.json                 # Variables Airflow
â””â”€â”€ requirements.txt                   # DÃ©pendances Python
```

### DAG Principal

**daily_market_ingestion.py**
```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator
from sectoral_operators import SectoralIngestionOperator, SectoralTransformOperator

default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'daily_market_ingestion',
    default_args=default_args,
    description='Pipeline quotidien d\'ingestion des donnÃ©es de marchÃ©',
    schedule_interval='0 6 * * 1-5',  # 6AM lundi-vendredi
    catchup=False,
    tags=['sectoral', 'daily', 'market-data']
)

# TÃ¢ches
ingest_stocks = SectoralIngestionOperator(
    task_id='ingest_stock_data',
    data_sources=['yahoo_finance', 'alpha_vantage'],
    symbols="{{ var.value.stock_symbols }}",
    s3_bucket="{{ var.value.s3_raw_bucket }}",
    dag=dag
)

validate_data = PythonOperator(
    task_id='validate_raw_data',
    python_callable=validate_stock_data,
    dag=dag
)

transform_data = SectoralTransformOperator(
    task_id='run_dbt_transformations',
    dbt_project_dir='/opt/airflow/dbt',
    profiles_dir='/opt/airflow/dbt/profiles',
    dag=dag
)

# DÃ©pendances
ingest_stocks >> validate_data >> transform_data
```

### Variables Airflow

```json
{
  "stock_symbols": [
    "AAPL", "MSFT", "GOOGL", "NVDA",
    "JNJ", "PFE", "UNH", "ABBV",
    "JPM", "BAC", "WFC", "GS",
    "XOM", "CVX", "COP", "SLB",
    "AMZN", "TSLA", "HD", "MCD"
  ],
  "s3_raw_bucket": "sectoral-dev-raw-data",
  "redshift_connection": "redshift_default",
  "notification_email": "admin@sectoral.com"
}
```

---

## ðŸ”§ ModÃ¨les dbt

### Structure dbt

```
dbt/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ profiles.yml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ _sources.yml
â”‚   â”‚   â”œâ”€â”€ stg_stock_prices.sql
â”‚   â”‚   â””â”€â”€ stg_market_indices.sql
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”œâ”€â”€ int_daily_returns.sql
â”‚   â”‚   â””â”€â”€ int_sector_aggregates.sql
â”‚   â””â”€â”€ marts/
â”‚       â”œâ”€â”€ financial/
â”‚       â”‚   â”œâ”€â”€ dim_sectors.sql
â”‚       â”‚   â”œâ”€â”€ fact_daily_performance.sql
â”‚       â”‚   â””â”€â”€ fact_sector_correlations.sql
â”‚       â””â”€â”€ analytics/
â”‚           â”œâ”€â”€ sector_performance_summary.sql
â”‚           â””â”€â”€ risk_metrics.sql
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ calculate_returns.sql
â”‚   â””â”€â”€ financial_metrics.sql
â”œâ”€â”€ tests/
â””â”€â”€ docs/
```

### Configuration dbt

**dbt_project.yml**
```yaml
name: 'sectoral'
version: '1.0.0'
config-version: 2

profile: 'sectoral'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]

models:
  sectoral:
    +materialized: table
    staging:
      +materialized: view
      +schema: staging
    intermediate:
      +materialized: ephemeral
    marts:
      +materialized: table
      +schema: marts
```

### ModÃ¨le Staging

**models/staging/stg_stock_prices.sql**
```sql
{{ config(materialized='view') }}

with raw_prices as (
    select
        symbol,
        date,
        open_price,
        high_price,
        low_price,
        close_price,
        volume,
        adjusted_close,
        created_at
    from {{ source('raw', 'stock_prices') }}
),

cleaned_prices as (
    select
        symbol,
        date::date as price_date,
        open_price::decimal(10,2) as open_price,
        high_price::decimal(10,2) as high_price,
        low_price::decimal(10,2) as low_price,
        close_price::decimal(10,2) as close_price,
        volume::bigint as volume,
        adjusted_close::decimal(10,2) as adjusted_close,
        created_at::timestamp as ingestion_timestamp
    from raw_prices
    where 
        close_price > 0
        and volume > 0
        and date >= '2020-01-01'
)

select * from cleaned_prices
```

### ModÃ¨le Mart

**models/marts/financial/fact_daily_performance.sql**
```sql
{{ config(materialized='table') }}

with daily_returns as (
    select
        symbol,
        price_date,
        close_price,
        {{ calculate_daily_return('close_price') }} as daily_return,
        {{ calculate_volatility('close_price', 30) }} as volatility_30d
    from {{ ref('stg_stock_prices') }}
),

sector_mapping as (
    select * from {{ ref('dim_sectors') }}
),

performance_metrics as (
    select
        dr.symbol,
        sm.sector_name,
        dr.price_date,
        dr.close_price,
        dr.daily_return,
        dr.volatility_30d,
        sum(dr.daily_return) over (
            partition by dr.symbol 
            order by dr.price_date 
            rows unbounded preceding
        ) as cumulative_return
    from daily_returns dr
    left join sector_mapping sm on dr.symbol = sm.symbol
)

select * from performance_metrics
```

### Macro FinanciÃ¨re

**macros/calculate_returns.sql**
```sql
{% macro calculate_daily_return(price_column) %}
    ({{ price_column }} - lag({{ price_column }}, 1) over (
        partition by symbol 
        order by price_date
    )) / lag({{ price_column }}, 1) over (
        partition by symbol 
        order by price_date
    )
{% endmacro %}

{% macro calculate_volatility(price_column, window_days) %}
    stddev(
        ({{ price_column }} - lag({{ price_column }}, 1) over (
            partition by symbol 
            order by price_date
        )) / lag({{ price_column }}, 1) over (
            partition by symbol 
            order by price_date
        )
    ) over (
        partition by symbol 
        order by price_date 
        rows {{ window_days - 1 }} preceding
    ) * sqrt(252)
{% endmacro %}
```

---

## ðŸ“Š Monitoring & Alertes

### MÃ©triques CloudWatch Custom

```python
import boto3

cloudwatch = boto3.client('cloudwatch')

def publish_data_quality_metrics(success_rate, records_processed):
    cloudwatch.put_metric_data(
        Namespace='Sectoral/DataQuality',
        MetricData=[
            {
                'MetricName': 'IngestionSuccessRate',
                'Value': success_rate,
                'Unit': 'Percent'
            },
            {
                'MetricName': 'RecordsProcessed',
                'Value': records_processed,
                'Unit': 'Count'
            }
        ]
    )
```

### Alertes SNS

```python
def send_alert(message, severity='INFO'):
    sns = boto3.client('sns')
    topic_arn = 'arn:aws:sns:eu-west-1:123456789:SectoralAlerts'
    
    sns.publish(
        TopicArn=topic_arn,
        Message=message,
        Subject=f'[{severity}] Sectoral Pipeline Alert'
    )
```

---

## ðŸ› Troubleshooting

### ProblÃ¨mes FrÃ©quents

#### 1. Terraform Apply Fail
```bash
# Erreur : "AccessDenied"
# Solution : VÃ©rifier les permissions IAM
aws iam list-attached-user-policies --user-name your-user
```

#### 2. Airflow DAG ne dÃ©marre pas
```bash
# VÃ©rifier les logs MWAA
aws logs describe-log-groups --log-group-name-prefix="/aws/mwaa/sectoral"

# Test syntaxe DAG
python -c "from airflow.models import DagBag; d = DagBag()"
```

#### 3. dbt Run Failed
```bash
# Debug dbt
dbt debug --profiles-dir ./profiles

# Test connexion Redshift
dbt run --select stg_stock_prices --profiles-dir ./profiles
```

#### 4. API Rate Limits
```bash
# Yahoo Finance : Pas de limite officielle mais throttling
# Solution : Retry avec backoff exponentiel

# Alpha Vantage : 5 calls/minute, 500/day
# Solution : GÃ©rer dans Airflow avec sensor
```

### Logs Utiles

```bash
# Logs Terraform
terraform plan -var-file="dev.tfvars" 2>&1 | tee terraform.log

# Logs Airflow Local
airflow scheduler --log-file scheduler.log

# Logs dbt
dbt run --log-level debug 2>&1 | tee dbt.log
```

---

## âœ… Bonnes Pratiques

### SÃ©curitÃ©
- âœ… Pas de credentials hardcodÃ©s
- âœ… Rotation des clÃ©s API
- âœ… Encryption S3 et Redshift
- âœ… VPC et Security Groups restrictifs

### Performance
- âœ… Partitioning S3 par date
- âœ… Compression Parquet
- âœ… Index Redshift sur colonnes frÃ©quentes
- âœ… Parallel processing Airflow

### QualitÃ© de Code
- âœ… Tests dbt (unique, not_null, relationships)
- âœ… Documentation models
- âœ… Version control (Git tags)
- âœ… Code review process

### Monitoring
- âœ… Alertes business (donnÃ©es manquantes > 20%)
- âœ… SLA monitoring (pipeline < 2h)
- âœ… Data freshness checks
- âœ… Cost monitoring AWS

---

## ðŸ“š Ressources ComplÃ©mentaires

- [AWS MWAA Documentation](https://docs.aws.amazon.com/mwaa/)
- [dbt Documentation](https://docs.getdbt.com/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Apache Airflow Guide](https://airflow.apache.org/docs/)

---

**ðŸ”— Retour au [README Principal](README.md)**